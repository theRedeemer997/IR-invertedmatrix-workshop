{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## ðŸ” Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed codeâ€”just like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data â€” such as AI agents.\n",
    "\n",
    "### ðŸ‘¥ Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## ðŸ”§ Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ðŸ§  Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## ðŸ§© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* â€“ Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* â€“ Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* â€“ Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* â€“ Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents('sample_docs/')\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cars', 'are', 'vehicles', 'designed', 'for', 'transportation', 'on', 'roads', 'they', 'typically', 'have', 'four', 'wheels', 'and', 'are', 'powered', 'by', 'internal', 'combustion', 'engines']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Test on one document\n",
    "tokens = tokenize(documents[0])\n",
    "print(tokens[:20])  # Preview first 20 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## ðŸ” Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'vehicl', 'design', 'transport', 'road', 'typic', 'four', 'wheel', 'power', 'intern', 'combust', 'engin', 'electr', 'motor', 'car', 'revolution', 'person', 'commerci', 'travel', 'make']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yogeshkumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(tokens)\n",
    "print(norm_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## ðŸ” Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'car': defaultdict(<class 'list'>, {0: [0, 14, 30, 32]}), 'vehicl': defaultdict(<class 'list'>, {0: [1]}), 'design': defaultdict(<class 'list'>, {0: [2], 1: [15]}), 'transport': defaultdict(<class 'list'>, {0: [3], 2: [0]}), 'road': defaultdict(<class 'list'>, {0: [4]}), 'typic': defaultdict(<class 'list'>, {0: [5]}), 'four': defaultdict(<class 'list'>, {0: [6]}), 'wheel': defaultdict(<class 'list'>, {0: [7]}), 'power': defaultdict(<class 'list'>, {0: [8]}), 'intern': defaultdict(<class 'list'>, {0: [9]}), 'combust': defaultdict(<class 'list'>, {0: [10]}), 'engin': defaultdict(<class 'list'>, {0: [11], 1: [17]}), 'electr': defaultdict(<class 'list'>, {0: [12]}), 'motor': defaultdict(<class 'list'>, {0: [13]}), 'revolution': defaultdict(<class 'list'>, {0: [15]}), 'person': defaultdict(<class 'list'>, {0: [16], 1: [7]}), 'commerci': defaultdict(<class 'list'>, {0: [17]}), 'travel': defaultdict(<class 'list'>, {0: [18]}), 'make': defaultdict(<class 'list'>, {0: [19]}), 'faster': defaultdict(<class 'list'>, {0: [20]}), 'conveni': defaultdict(<class 'list'>, {0: [21]}), 'year': defaultdict(<class 'list'>, {0: [22]}), 'advanc': defaultdict(<class 'list'>, {0: [23]}), 'technolog': defaultdict(<class 'list'>, {0: [24]}), 'led': defaultdict(<class 'list'>, {0: [25]}), 'safer': defaultdict(<class 'list'>, {0: [26]}), 'effici': defaultdict(<class 'list'>, {0: [27]}), 'environment': defaultdict(<class 'list'>, {0: [28]}), 'friendli': defaultdict(<class 'list'>, {0: [29]}), 'today': defaultdict(<class 'list'>, {0: [31]}), 'play': defaultdict(<class 'list'>, {0: [33]}), 'vital': defaultdict(<class 'list'>, {0: [34]}), 'role': defaultdict(<class 'list'>, {0: [35]}), 'daili': defaultdict(<class 'list'>, {0: [36]}), 'life': defaultdict(<class 'list'>, {0: [37]}), 'connect': defaultdict(<class 'list'>, {0: [38]}), 'peopl': defaultdict(<class 'list'>, {0: [39]}), 'place': defaultdict(<class 'list'>, {0: [40]}), 'around': defaultdict(<class 'list'>, {0: [41]}), 'world': defaultdict(<class 'list'>, {0: [42]}), 'invent': defaultdict(<class 'list'>, {1: [0, 2]}), 'comput': defaultdict(<class 'list'>, {1: [1, 3, 14, 22, 33, 42, 47, 61, 63]}), 'cannot': defaultdict(<class 'list'>, {1: [4]}), 'attribut': defaultdict(<class 'list'>, {1: [5]}), 'singl': defaultdict(<class 'list'>, {1: [6]}), 'howev': defaultdict(<class 'list'>, {1: [8]}), 'charl': defaultdict(<class 'list'>, {1: [9]}), 'babbag': defaultdict(<class 'list'>, {1: [10]}), 'often': defaultdict(<class 'list'>, {1: [11]}), 'consid': defaultdict(<class 'list'>, {1: [12]}), 'father': defaultdict(<class 'list'>, {1: [13]}), 'analyt': defaultdict(<class 'list'>, {1: [16]}), '1830': defaultdict(<class 'list'>, {1: [18]}), 'conceptu': defaultdict(<class 'list'>, {1: [19]}), 'precursor': defaultdict(<class 'list'>, {1: [20]}), 'modern': defaultdict(<class 'list'>, {1: [21]}), 'key': defaultdict(<class 'list'>, {1: [23]}), 'contributor': defaultdict(<class 'list'>, {1: [24]}), 'includ': defaultdict(<class 'list'>, {1: [25]}), 'alan': defaultdict(<class 'list'>, {1: [26]}), 'ture': defaultdict(<class 'list'>, {1: [27, 30]}), 'develop': defaultdict(<class 'list'>, {1: [28, 53, 62]}), 'concept': defaultdict(<class 'list'>, {1: [29]}), 'machin': defaultdict(<class 'list'>, {1: [31]}), 'foundat': defaultdict(<class 'list'>, {1: [32]}), 'scienc': defaultdict(<class 'list'>, {1: [34]}), 'john': defaultdict(<class 'list'>, {1: [35, 48, 51]}), 'atanasoff': defaultdict(<class 'list'>, {1: [36, 40]}), 'clifford': defaultdict(<class 'list'>, {1: [37]}), 'berri': defaultdict(<class 'list'>, {1: [38, 41]}), 'built': defaultdict(<class 'list'>, {1: [39]}), 'abc': defaultdict(<class 'list'>, {1: [43]}), 'earli': defaultdict(<class 'list'>, {1: [44]}), 'electron': defaultdict(<class 'list'>, {1: [45, 59]}), 'digit': defaultdict(<class 'list'>, {1: [46, 60]}), 'presper': defaultdict(<class 'list'>, {1: [49]}), 'eckert': defaultdict(<class 'list'>, {1: [50]}), 'mauchli': defaultdict(<class 'list'>, {1: [52]}), 'eniac': defaultdict(<class 'list'>, {1: [54]}), 'one': defaultdict(<class 'list'>, {1: [55]}), 'first': defaultdict(<class 'list'>, {1: [56]}), 'gener': defaultdict(<class 'list'>, {1: [57]}), 'purpos': defaultdict(<class 'list'>, {1: [58]}), 'collabor': defaultdict(<class 'list'>, {1: [64]}), 'process': defaultdict(<class 'list'>, {1: [65]}), 'involv': defaultdict(<class 'list'>, {1: [66]}), 'mani': defaultdict(<class 'list'>, {1: [67]}), 'inventor': defaultdict(<class 'list'>, {1: [68]}), 'scientist': defaultdict(<class 'list'>, {1: [69]}), 'time': defaultdict(<class 'list'>, {1: [70]})}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_position_inverted_index(documents):\n",
    "    index = defaultdict(lambda: defaultdict(list))  # term -> {doc_id: [positions]}\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))  # assumes you already have tokenize and normalize_tokens()\n",
    "        for position, token in enumerate(tokens):\n",
    "            index[token][doc_id].append(position)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_position_inverted_index(documents)\n",
    "print(dict(inverted_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_query_search(phrase, inverted_index):\n",
    "    terms = phrase.lower().split()\n",
    "    if not terms:\n",
    "        return []\n",
    "\n",
    "    postings = [inverted_index.get(term, {}) for term in terms]\n",
    "\n",
    "    common_docs = set(postings[0].keys())\n",
    "    for p in postings[1:]:\n",
    "        common_docs &= set(p.keys())\n",
    "\n",
    "    results = []\n",
    "    for doc_id in common_docs:\n",
    "        positions_lists = [p[doc_id] for p in postings]\n",
    "        for pos in positions_lists[0]:\n",
    "            if all((pos + i) in positions_lists[i] for i in range(1, len(terms))):\n",
    "                results.append(doc_id)\n",
    "                break\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a0e3f82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query1: [0, 1]\n",
      "Results for query2: []\n"
     ]
    }
   ],
   "source": [
    "query1 = \"design\"\n",
    "query2 = \"transport design\"\n",
    "\n",
    "matching_docs1 = phrase_query_search(query1, inverted_index)\n",
    "matching_docs2 = phrase_query_search(query2, inverted_index)\n",
    "\n",
    "print(\"Results for query1:\", matching_docs1)\n",
    "print(\"Results for query2:\", matching_docs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c249bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def phrase_query_search_stem(phrase, inverted_index):\n",
    "    # Tokenize, normalize, and stem the query\n",
    "    tokens = normalize_tokens(tokenize(phrase))   # assuming these are defined\n",
    "    stemmed_terms = stem_tokens(tokens)    \n",
    "    print(stemmed_terms)# apply the same stemming as for documents\n",
    "\n",
    "    if not stemmed_terms:\n",
    "        return []\n",
    "\n",
    "    postings = [inverted_index.get(term, {}) for term in stemmed_terms]\n",
    "\n",
    "    common_docs = set(postings[0].keys())\n",
    "    for p in postings[1:]:\n",
    "        common_docs &= set(p.keys())\n",
    "\n",
    "    results = []\n",
    "    for doc_id in common_docs:\n",
    "        positions_lists = [p[doc_id] for p in postings]\n",
    "        for pos in positions_lists[0]:\n",
    "            if all((pos + i) in positions_lists[i] for i in range(1, len(stemmed_terms))):\n",
    "                results.append(doc_id)\n",
    "                break\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "13007184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['design']\n",
      "['transport', 'dese']\n",
      "Query 1 matched documents: [0, 1]\n",
      "Query 2 matched documents: []\n"
     ]
    }
   ],
   "source": [
    "query1 = \"design\"\n",
    "query2 = \"transportion desing\"\n",
    "\n",
    "matching_docs1 = phrase_query_search_stem(query1, inverted_index)\n",
    "matching_docs2 = phrase_query_search_stem(query2, inverted_index)\n",
    "\n",
    "print(\"Query 1 matched documents:\", matching_docs1)\n",
    "print(\"Query 2 matched documents:\", matching_docs2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
